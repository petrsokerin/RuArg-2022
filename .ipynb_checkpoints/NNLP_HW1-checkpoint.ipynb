{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507a706f",
   "metadata": {
    "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0"
   },
   "source": [
    "# 1. Information about the submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6532ca",
   "metadata": {
    "id": "e37cb5bb-f3d0-4c11-a1dc-2490a208fcd3"
   },
   "source": [
    "## 1.1 Name and number of the assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb60bc",
   "metadata": {
    "id": "4e9d00b8-f3e5-4a44-bcc6-35cdd60767a9"
   },
   "source": [
    "Text categorization: argument mining, NNLP2022 HW1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6724a60",
   "metadata": {
    "id": "64ba7f63-66ec-4691-a5d2-17f4679e298d"
   },
   "source": [
    "## 1.2 Student name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ea91c",
   "metadata": {
    "id": "cc8a4e09-62cc-43fd-a7a7-3e9d55ec13b2"
   },
   "source": [
    "Petr Sokerin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc601482",
   "metadata": {
    "id": "8a46ab45-d215-41af-b910-63ff4a215a07"
   },
   "source": [
    "## 1.3 Codalab user ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c6a6e",
   "metadata": {
    "id": "b15cd6b5-8e20-4287-b6ea-a7b0904b355a"
   },
   "source": [
    "petr_sokerin\n",
    "\n",
    "https://codalab.lisn.upsaclay.fr/competitions/786"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8383e0",
   "metadata": {
    "id": "70456c74-8e1f-4da0-bebe-fbceee169115"
   },
   "source": [
    "## 1.4 Additional comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425cb6d1",
   "metadata": {
    "id": "6b810ac6-7739-4f7f-8bea-dbf1198570ea"
   },
   "source": [
    "***Enter here** any additional comments which you would like to communicate to a TA who is going to grade this work not related to the content of your submission.\n",
    "\n",
    "please, run this notebook in zip archive with data and puctures for report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6face8ac",
   "metadata": {
    "id": "1af498ab-3c00-4d36-a962-c947862fede8"
   },
   "source": [
    "# 2. Technical Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109d294f",
   "metadata": {
    "id": "9b3c18a6-868b-4357-a308-7f6dff05c3d0"
   },
   "source": [
    "*Use Section 2 to describe results of your experiments as you would do writing a paper about your results. DO NOT insert code in this part. Only insert plots and tables summarizing results as needed. Use formulas if needed do described your methodology. The code is provided in Section 3.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ed226",
   "metadata": {
    "id": "061f71b9-114a-4cb0-b531-5711970317bf"
   },
   "source": [
    "## 2.1 Methodology "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6531526",
   "metadata": {},
   "source": [
    "In this task I was aimed to provide multiclass text classification with 6 different target values (4 classes in each other).\n",
    "\n",
    "For solving this task I used different feature extraction methods and different model for prediction.\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "As we can knoow, to succeed in NLP task you need to pay a lot of attention to preprocessing of your data. In me preprocessing I remove puntcuation, reduce words to lowercase, drop stop words, use tockenization with Tweet Tockenizer from NLTK package and lemmatization with stemmer pymystem3. \n",
    "\n",
    "Besides. ALmost classes in this tasks ater imbalanced so I used oversampling with RandomOverSampler on train set to achieve better result.\n",
    "\n",
    "And last, but onot least: to succeed in this task I prefer to solve each of 6 tasks as separate tasks and for each task choose best preprocessing with best model.\n",
    "\n",
    "For data validation I used train test split on train set to not send results each time for validation. The size of validation was chosen as 0.25.\n",
    "\n",
    "\n",
    "### Feature extraction methods:\n",
    "\n",
    "The next step is feature extraction, or vectorization of text. I used 4 appraches to do it that are work so-so and 1 absolutely failing approach. \n",
    "\n",
    "***1) TF-IDF*** (TF — term frequency, IDF — inverse document frequency). This is method of vectorization of the text that include to part: TF and IDF.\n",
    "\n",
    "$$tf(t,d) = \\frac{f_{t,d}}{\\Sigma_{t^{'}\\in d} f_{t^{'},d}}$$\n",
    "\n",
    "where $f_{t,d}$ is the raw count of a term in a document, i.e., the number of times that term t occurs in document d. \n",
    "\n",
    "$$idf(t,D) = log \\frac{N}{|d \\in D : t \\in d|}$$\n",
    "\n",
    "where N - total number of documents in corpus, D - corpus.\n",
    "\n",
    "I choose sklearn implementation of this method. Besides, I used ngram_range=(1, 2), max_features=150000 to reduce the size of a matrix and for better representation of the same words.The advantage of this method is quiet well sparse representation of texts, but disadvantage is that representation is sparse. To fix it problem I tried to apply SVD method in my next feature extraction approach. \n",
    "\n",
    "***2) TF-IDF with SVD*** In this method I tried to apply svd to gotten TF-IDF matrix and take matrix U as document embeddings. The size of rank was choosen as 1000. \n",
    "\n",
    "$ A_{TF-IDF} = U * \\Sigma * V^T$\n",
    "\n",
    "\n",
    "***3) Bert***. As a state-of-the-art model for vectorisation I prefer to use Bert model trained on different russian datasets. I took bert implementation from sberbank AI (sberbank-ai/sbert_large_nlu_ru). Vector size of embeddings is equal 1024.This model was used in 2 approaches: generally I feed the model the sensense and get back embedding for the entire text. However, to train RNN models I used embeddings of each word in the document to feed in LSTM layer.  \n",
    "\n",
    "***4) Word2vec with fasttext***. I also used pretrained model Word2Vec with from gensym package with data 'ru_fasttext_model/model.model'. As in previous case I generally tried to find embedding for the document by calculation average embedding for each tocken of the document. However, for RNN case I also stored all embeddings to feed it in neural network.\n",
    "\n",
    "***5) Self-trained embeddings***. Last (and the least) way to get embeddings is to train embeddings with embedding layer of Pytorch model, but it provide awful result because we have too small corpus to train neural network embeddings(\n",
    "\n",
    "\n",
    "### Modelling:\n",
    "\n",
    "#### classical ML:\n",
    "\n",
    "We can solve the task of classification as with classical ML moodels as with neural networks.\n",
    "\n",
    "As classical ML models I used:\n",
    "\n",
    "- sklearn logistic regression with basic params\n",
    "- sklearn RandomForest with basic params\n",
    "- sklearn KNN with basic params\n",
    "- sklearn SVM with basic params\n",
    "- XGBoost XGBClassifier with basic params \n",
    "\n",
    "For this models I used basic preprocessing and all feature extraction methods except self-trained embeddings. I would like to emphasize that for each task I trained special model. \n",
    "\n",
    "#### Neural Network:\n",
    "\n",
    "As neural networks I used 2 models: \n",
    "\n",
    "- MLP model with feature extraction for entire text. As architecture of this models I used 2 fully connected models. The size of the first layer is equal of enter data size, the second layer size 256. Between 2 layers I udes ReLU activation and dropout with probability 0.2. As optimizer I used Adam, as loss cross entripy loss, as learning rate 0.001 with scheduler Stepscheduler(n_steps=5, gamma=0.1). I trained this neural network with TF-IDF, and Bert embeddings because word2vec embeddig as average ambedding for entire text and SVD with TF-IDF demonstrate worse result on classical ML models. I trained model on BERT embeddings with 15 epochs and TF-IDF. \n",
    "\n",
    "\n",
    "- LSTM model with Bert feature extraction with RNN (LSTM) model. As an architecture of model I use BiLSTM layer with 2 layers and output size 2048. After that I have 2 fully connected layers with relu activation and dropout with probability 0.2 as in previous case. As optimizer I used Adam with weights decay 0.0001, as loss cross entripy loss, as learning rate 0.001 with scheduler Stepscheduler(n_steps=4, gamma=0.1). As data I used bert embeddings for all words as a sequential.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff7af3",
   "metadata": {
    "id": "afe27e49-10c7-4c12-adea-48b0a05a5681"
   },
   "source": [
    "## 2.2 Discussion of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376278e",
   "metadata": {},
   "source": [
    "<img src=\"results/Picture1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c217432",
   "metadata": {},
   "source": [
    "In the end I get ambiguous results. As we can see from table above, that provide information about F1-measure for each task for different models, I get better results with TF-IDF feature extraction methods and BERT embeddings. Actually first time I suppose that BERT embeddings should provide better results, but TF-IDF despite it's simplicity provide pretty nice results. The reason of it can be specific area of texts - medicine spheare and pretrained embeddings were needed fine-tuning. On the other hand, dataset is too small for it. SVD wih TF-IDF can lost too much information because of usage pretty big, but not big enouth rank for decomposition (it was 1000).\n",
    "\n",
    "I would like to emphasize that my strategy was diversification and using different models for each other classification task. So I find best model for each task:\n",
    "\n",
    "- 'masks_stance': feature extraction 'TF-IDF', model logistic regression\n",
    "- 'masks_argument':  feature extraction 'TF-IDF', model logistic regression\n",
    "- 'quarantine_stance': feature extraction'TF-IDF', model logistic regression\n",
    "- 'quarantine_argument':  feature extraction 'BERT', model support-vector-machine\n",
    "- 'vaccines_stance':  feature extraction 'TF-IDF', model Neural network MLP\n",
    "- 'vaccines_argument':  feature extraction 'BERT', model Neural network LSTM\n",
    "\n",
    "The reason of success of logistic regression can be that this method can work quiet well with TF-IDF in text classification task. TF-IDF also perform quiet well with MLP NN wthat have the same idea as logistic regression. BERT feature extraction is much harder to work with logistic regression, so it perform better with not linear SVM and Recurrent Neural network. \n",
    "\n",
    "As potential improvement, pretrained transormer model can be used for this task and other embeddings method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab3f669",
   "metadata": {
    "id": "194fecf1-e044-4210-a54b-aefbf4b4eebe"
   },
   "source": [
    "# 3. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec423f8",
   "metadata": {
    "id": "a33ff9bd-62c6-4a63-8600-b1651420fee1"
   },
   "source": [
    "*Enter here all code used to produce your results submitted to Codalab. Add some comments and subsections to navigate though your solution.*\n",
    "\n",
    "*In this part you are expected to develop yourself a solution of the task and provide a reproducible code:*\n",
    "- *Using Python 3;*\n",
    "- *Contains code for installation of all dependencies;*\n",
    "- *Contains code for downloading of all the datasets used*;\n",
    "- *Contains the code for reproducing your results (in other words, if a tester downloads your notebook she should be able to run cell-by-cell the code and obtain your experimental results as described in the methodology section)*.\n",
    "\n",
    "\n",
    "*As a result, you code will be graded according to these criteria:*\n",
    "- ***Readability**: your code should be well-structured preferably with indicated parts of your approach (Preprocessing, Model training, Evaluation, etc.).*\n",
    "- ***Reproducibility**: your code should be reproduced without any mistakes with “Run all” mode (obtaining experimental part).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6e6b46",
   "metadata": {
    "id": "dff93e37-3a24-40ab-87db-16b537aad3f6"
   },
   "source": [
    "## 3.1 Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2315757d",
   "metadata": {
    "id": "73daa932-114b-4e28-9141-13b57c729435",
    "outputId": "59ffda92-c218-4c0f-9fa9-bed804d036e3"
   },
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!pip install imblearn\n",
    "\n",
    "!wget http://vectors.nlpl.eu/repository/20/214.zip\n",
    "!unzip 214.zip -d ru_fasttext_model\n",
    "\n",
    "!pip install --upgrade torch torchtext\n",
    "!pip install gensim\n",
    "!pip install pymystem3\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "683c0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the locations of auxiliary libraries and datasets.\n",
    "# `AUX_DATA_ROOT` is where 'tiny-imagenet-2022.zip' is.\n",
    "\n",
    "# Detect if we are in Google Colaboratory\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b855a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /cephfs/projects/psoker/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import string\n",
    "from pymystem3 import Mystem\n",
    "import nltk\n",
    "nltk.download('stopwords')  \n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47832e94",
   "metadata": {
    "id": "1b3c19fa-f883-4675-9506-85c4f02f0af9"
   },
   "source": [
    "## 3.2 Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "476e6e69",
   "metadata": {
    "id": "8f366b0f-7d0b-44e0-bb82-0d82c3ea1bb1",
    "outputId": "335d5258-1f0c-4c9b-f6ce-c1278c7ad5bd"
   },
   "outputs": [],
   "source": [
    "# !wget -O train.tsv https://raw.githubusercontent.com/dialogue-evaluation/RuArg/main/data/train.tsv\n",
    "# !wget -O val_empty.tsv https://raw.githubusercontent.com/dialogue-evaluation/RuArg/main/data/val_empty.tsv\n",
    "# !wget -O test-no_labels.tsv https://raw.githubusercontent.com/dialogue-evaluation/RuArg/main/data/test-no_labels.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0680dd53",
   "metadata": {
    "id": "ed8aa6f0-79e0-4c7d-b6bc-2dcd48382af2",
    "outputId": "2f0dc06b-b131-49e9-e633-100f0567c3de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>masks_stance</th>\n",
       "      <th>masks_argument</th>\n",
       "      <th>quarantine_stance</th>\n",
       "      <th>quarantine_argument</th>\n",
       "      <th>vaccines_stance</th>\n",
       "      <th>vaccines_argument</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2142</th>\n",
       "      <td>22503</td>\n",
       "      <td>При этом маски никто не носит, руки не дезинфи...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>28102</td>\n",
       "      <td>Вы у медиков - хирургов спросите, как они по 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>18525</td>\n",
       "      <td>Ну, так никто не предполагал, что толпы обезум...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>18295</td>\n",
       "      <td>Кто будет обеспечивать людей которые сидят на ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2229</th>\n",
       "      <td>22705</td>\n",
       "      <td>Она налицо: все люди в масках, график заболева...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text_id                                               text  \\\n",
       "2142    22503  При этом маски никто не носит, руки не дезинфи...   \n",
       "4367    28102  Вы у медиков - хирургов спросите, как они по 1...   \n",
       "564     18525  Ну, так никто не предполагал, что толпы обезум...   \n",
       "478     18295  Кто будет обеспечивать людей которые сидят на ...   \n",
       "2229    22705  Она налицо: все люди в масках, график заболева...   \n",
       "\n",
       "      masks_stance  masks_argument  quarantine_stance  quarantine_argument  \\\n",
       "2142             1               1                 -1                   -1   \n",
       "4367             1               1                 -1                   -1   \n",
       "564             -1              -1                  1                    1   \n",
       "478             -1              -1                  0                    0   \n",
       "2229             1               0                 -1                   -1   \n",
       "\n",
       "      vaccines_stance  vaccines_argument  \n",
       "2142               -1                 -1  \n",
       "4367               -1                 -1  \n",
       "564                -1                 -1  \n",
       "478                -1                 -1  \n",
       "2229               -1                 -1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RS = 42\n",
    "\n",
    "train_df = pd.read_csv('data/train_all.tsv', sep='\\t')\n",
    "train_df, val_df = train_test_split(train_df, random_state=RS)\n",
    "\n",
    "test_df = pd.read_csv('data/test-no_labels.tsv', sep='\\t')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4565afe",
   "metadata": {
    "id": "791dc0e7-337d-46ad-96a3-543a732f19e2"
   },
   "source": [
    "## 3.3 Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0050062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(texts, stop_words, tockenize=False):\n",
    "    \n",
    "    result = []\n",
    "    for i in tqdm(range(len(texts))):\n",
    "        text = texts[i]\n",
    "        \n",
    "        text = str(text).lower().strip() #lowercase  \n",
    "        text = \"\".join([char for char in text if char not in string.punctuation]) #removing punctuation\n",
    "        \n",
    "        lem_text = mystem.lemmatize(text) #lemmatization\n",
    "        tockens = [tocken for tocken in lem_text if tocken not in stop_words and tocken != ' '] #dropping stop words\n",
    "        text = ' '.join(tockens).strip()\n",
    "        \n",
    "        if tockenize:\n",
    "            result.append(tokenizer.tokenize(text))\n",
    "        else:\n",
    "            result.append(text)\n",
    "            \n",
    "    return np.array(result) \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30c36de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f951b899bb7940038bffb375dd0d1670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b7ee57099948adae217e1c3cbd6865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f38a96be5f434db4b4a6a7d5a25432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cc60772dbb439e9251c08840788089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9452fb2cbfae411eaa10e2ab119b8ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f074320f2e14a4587f6ddaac5fe5cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stopwords = set(nltk_stopwords.words('russian'))\n",
    "mystem = Mystem()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "split_prepr_text_dict= {}\n",
    "split_prepr_text_dict['train'] = preprocessing(train_df['text'].values, stopwords)\n",
    "split_prepr_text_dict['val'] = preprocessing(val_df['text'].values, stopwords)\n",
    "split_prepr_text_dict['test'] = preprocessing(test_df['text'].values, stopwords)\n",
    "\n",
    "split_prepr_tockens_dict= {}\n",
    "split_prepr_tockens_dict['train'] = preprocessing(train_df['text'].values, stopwords, tockenize=True)\n",
    "split_prepr_tockens_dict['val'] = preprocessing(val_df['text'].values, stopwords, tockenize=True)\n",
    "split_prepr_tockens_dict['test'] = preprocessing(test_df['text'].values, stopwords, tockenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ec5619e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>masks_stance</th>\n",
       "      <th>masks_argument</th>\n",
       "      <th>quarantine_stance</th>\n",
       "      <th>quarantine_argument</th>\n",
       "      <th>vaccines_stance</th>\n",
       "      <th>vaccines_argument</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5857</th>\n",
       "      <td>31889</td>\n",
       "      <td>Да и хрен то с этим , живы после вакцинации , ...</td>\n",
       "      <td>__class__0</td>\n",
       "      <td>__class__0</td>\n",
       "      <td>__class__0</td>\n",
       "      <td>__class__0</td>\n",
       "      <td>__class__3</td>\n",
       "      <td>__class__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4220</th>\n",
       "      <td>27739</td>\n",
       "      <td>Только раньше маски должны были надевать больн...</td>\n",
       "      <td>__class__3</td>\n",
       "      <td>__class__2</td>\n",
       "      <td>__class__0</td>\n",
       "      <td>__class__0</td>\n",
       "      <td>__class__0</td>\n",
       "      <td>__class__0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>23343</td>\n",
       "      <td>[USER], увлажняйте глаз, если появился синдром...</td>\n",
       "      <td>__class__3</td>\n",
       "      <td>__class__3</td>\n",
       "      <td>__class__0</td>\n",
       "      <td>__class__0</td>\n",
       "      <td>__class__0</td>\n",
       "      <td>__class__0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>21190</td>\n",
       "      <td>В торговые центры на карантине пойдут те, кто ...</td>\n",
       "      <td>__class__0</td>\n",
       "      <td>__class__0</td>\n",
       "      <td>__class__2</td>\n",
       "      <td>__class__2</td>\n",
       "      <td>__class__0</td>\n",
       "      <td>__class__0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4149</th>\n",
       "      <td>27563</td>\n",
       "      <td>А маски реально помогают если их правильно нос...</td>\n",
       "      <td>__class__3</td>\n",
       "      <td>__class__3</td>\n",
       "      <td>__class__0</td>\n",
       "      <td>__class__0</td>\n",
       "      <td>__class__0</td>\n",
       "      <td>__class__0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text_id                                               text masks_stance  \\\n",
       "5857    31889  Да и хрен то с этим , живы после вакцинации , ...   __class__0   \n",
       "4220    27739  Только раньше маски должны были надевать больн...   __class__3   \n",
       "2484    23343  [USER], увлажняйте глаз, если появился синдром...   __class__3   \n",
       "1615    21190  В торговые центры на карантине пойдут те, кто ...   __class__0   \n",
       "4149    27563  А маски реально помогают если их правильно нос...   __class__3   \n",
       "\n",
       "     masks_argument quarantine_stance quarantine_argument vaccines_stance  \\\n",
       "5857     __class__0        __class__0          __class__0      __class__3   \n",
       "4220     __class__2        __class__0          __class__0      __class__0   \n",
       "2484     __class__3        __class__0          __class__0      __class__0   \n",
       "1615     __class__0        __class__2          __class__2      __class__0   \n",
       "4149     __class__3        __class__0          __class__0      __class__0   \n",
       "\n",
       "     vaccines_argument  \n",
       "5857        __class__2  \n",
       "4220        __class__0  \n",
       "2484        __class__0  \n",
       "1615        __class__0  \n",
       "4149        __class__0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change labels\n",
    "def categorizer(data, class_name):\n",
    "    # Select required columns\n",
    "    data = data[['text_id', 'text', f'{class_name}_stance', f'{class_name}_argument']]\n",
    "\n",
    "    # Set your model output as categorical and save in new label col\n",
    "    data['stance_label'] = pd.Categorical(data[f'{class_name}_stance'])\n",
    "    data['argument_label'] = pd.Categorical(data[f'{class_name}_argument'])\n",
    "\n",
    "    # Transform your output to numeric\n",
    "    data[f'{class_name}_stance'] = data['stance_label'].cat.codes\n",
    "    data[f'{class_name}_argument'] = data['argument_label'].cat.codes\n",
    "    return data\n",
    "\n",
    "label = '__class__'\n",
    "for class_name in [\"quarantine\", \"vaccines\",  \"masks\"]:\n",
    "    tmp_data = categorizer(train_df, class_name=class_name)\n",
    "    train_data = tmp_data[['text', f'{class_name}_stance', f'{class_name}_argument']]\n",
    "    for case in ['stance', 'argument']:\n",
    "        train_df[f'{class_name}_{case}'] = label + train_data[f'{class_name}_{case}'].astype(str)\n",
    "        \n",
    "    tmp_data = categorizer(val_df, class_name=class_name)\n",
    "    val_data = tmp_data[['text', f'{class_name}_stance', f'{class_name}_argument']]\n",
    "    for case in ['stance', 'argument']:\n",
    "        val_df[f'{class_name}_{case}'] = label + val_data[f'{class_name}_{case}'].astype(str)\n",
    "        \n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "913d8557",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_preproc_data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2be06f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_original_dict= {}\n",
    "split_original_dict['train'] = train_df[[col for col in train_df.columns if 'text' not in col]]\n",
    "split_original_dict['val'] = val_df[[col for col in train_df.columns if 'text' not in col]]\n",
    "split_original_dict['test'] = test_df[[col for col in train_df.columns if 'text' not in col]]\n",
    "\n",
    "split_text_dict= {}\n",
    "split_text_dict['train'] = train_df['text']\n",
    "split_text_dict['val'] = val_df['text']\n",
    "split_text_dict['test'] = test_df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1efd10e",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4fdc01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transformer = TfidfVectorizer(stop_words=stopwords, ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
    "\n",
    "split_dict= {}\n",
    "\n",
    "split_dict['train'] = text_transformer.fit_transform(split_prepr_text_dict['train'])\n",
    "split_dict['val'] = text_transformer.transform(split_prepr_text_dict['val'])\n",
    "split_dict['test'] = text_transformer.transform(split_prepr_text_dict['test'])\n",
    "\n",
    "dict_preproc_data['TF-IDF'] = split_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32e7c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 1000\n",
    "\n",
    "split_dict_tf_idf= dict_preproc_data['TF-IDF']\n",
    "split_dict= {}\n",
    "\n",
    "split_dict = {split:scipy.sparse.linalg.svds(split_dict_tf_idf[split], k=emb_size)[0] \n",
    "              for split in split_dict_tf_idf}\n",
    "\n",
    "dict_preproc_data['TF-IDF-SVD'] = split_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78d778d",
   "metadata": {},
   "source": [
    "# Word2Vec fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73a93ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "ru_fasttext_model = gensim.models.KeyedVectors.load('ru_fasttext_model/model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b77e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_embedding(model, phrase_list):\n",
    "\n",
    "    vector = np.zeros([model.vector_size], dtype='float32')\n",
    "    used_words = 0\n",
    "    \n",
    "    for word in phrase_list:\n",
    "        if word in model:\n",
    "            vector += model.get_vector(word)\n",
    "            used_words += 1\n",
    "    \n",
    "    if used_words > 0:\n",
    "        vector = vector / used_words\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e8ce76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dict= {split:np.array([get_phrase_embedding(ru_fasttext_model, elem) for elem in data])\n",
    "            for split, data in split_text_dict.items()}\n",
    "\n",
    "\n",
    "dict_preproc_data['word2vec'] = split_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82fa8fe",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32dda2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")\n",
    "model_bert = AutoModel.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09a8071f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024,)\n"
     ]
    }
   ],
   "source": [
    "model_bert.cuda(device=0) \n",
    "import torch\n",
    "\n",
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True,max_length=250, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()\n",
    "\n",
    "print(embed_bert_cls('привет мир', model_bert, tokenizer_bert).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fee86608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************  train  *******************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713784c5f65c4c2a854aceef2fe195f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************  val  *******************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2af9993f494bc0a452796e9f436bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************  test  *******************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72fef8cc3b64f3c9e93ff80f434b2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb_dict=dict()\n",
    "\n",
    "for split, texts in split_prepr_text_dict.items():\n",
    "    \n",
    "    print(f'******************  {split}  *******************')\n",
    "    res = []\n",
    "    #texts = split_text_dict[split].values.tolist()\n",
    "    for text in tqdm(texts):\n",
    "        try:\n",
    "            pr_text = embed_bert_cls(text, model_bert, tokenizer_bert)\n",
    "        except:\n",
    "            print(text)\n",
    "        res.append(pr_text)\n",
    "    emb_dict[split] = np.array(res)\n",
    "    \n",
    "dict_preproc_data['BERT'] = emb_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4febf5fa",
   "metadata": {},
   "source": [
    "# 4. ML prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aee81bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TF-IDF': {}, 'TF-IDF-SVD': {}, 'word2vec': {}, 'BERT': {}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dict = {f_e:dict() for f_e in dict_preproc_data}\n",
    "pred_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d8a83",
   "metadata": {},
   "source": [
    "## Classical ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ffeff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_prediction = dict()\n",
    "\n",
    "classical_ml_models = {\n",
    "    'xgb':XGBClassifier(verbosity = 0, random_state=0xC0FFEE, n_jobs=None),\n",
    "    'rf':RandomForestClassifier(random_state=0xC0FFEE),\n",
    "    'lr':LogisticRegression(random_state=0xC0FFEE),\n",
    "    'svm':SVC(random_state=0xC0FFEE),\n",
    "    'knn':KNeighborsClassifier(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010c226c",
   "metadata": {},
   "source": [
    "### grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b17c23a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_ml_params = {\n",
    "    \n",
    "'xgb':{\n",
    "    \"learning_rate\" : [0.001, 0.01, 0.1, 1],\n",
    "    \"reg_lambda\": [0.001, 0.01, 0.1, ],\n",
    "    'n_estimators':range(50,500,100),\n",
    "    'min_samples_leaf': range(1, 17, 5),\n",
    "},\n",
    "    \n",
    "'rf':{\n",
    "    'n_estimators':range(50,500,50),\n",
    "    'min_samples_leaf': range(1, 16, 3),\n",
    "    'min_samples_split': range(4, 17, 4),\n",
    "    'max_depth': range(1, 18, 3), \n",
    "},\n",
    "    \n",
    "'lr':{\n",
    "    'C': [0.1, 0.5, 1, 5, 10, 30],\n",
    "    'penalty': ['l2', 'l1'],\n",
    "    'multi_class': ['auto', 'ovr', 'multinomial'],\n",
    "    'solver': ['lbfgs']\n",
    "},\n",
    "    \n",
    "'svm':{\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "},\n",
    "\n",
    "'knn':{\n",
    "    'n_neighbors': [3, 5, 9, 11, 15, 17, 19],\n",
    "    'weights': [\"uniform\", \"distance\"],\n",
    "    'metric': [\"euclidean\", \"manhattan\"]\n",
    "}\n",
    "}\n",
    "\n",
    "def grid_search(split_dict, classical_ml_models, classical_ml_params):\n",
    "    ros = RandomOverSampler(random_state=RS)\n",
    "    \n",
    "    dict_best_params = dict() #keys = model_name\n",
    "    for model_name in classical_ml_models:\n",
    "        dict_best_params_l1 = dict()\n",
    "        print(model_name, ' searching params')\n",
    "        for class_name in tqdm([\"quarantine\", \"vaccines\",  \"masks\"]):\n",
    "            dict_best_params_l2 = dict()\n",
    "            for case in ['stance', 'argument']:\n",
    "                x_ros, y_ros = ros.fit_resample(split_dict['train'], split_original_dict['train'][f'{class_name}_{case}'])\n",
    "                model = classical_ml_models[model_name]\n",
    "                params_grid = classical_ml_params[model_name]\n",
    "                \n",
    "                grid_model = GridSearchCV(model, param_grid=params_grid, cv=5, n_jobs=1, verbose=0, scoring='f1_weighted')\n",
    "                grid_model.fit(x_ros, y_ros)\n",
    "                dict_best_params_l2[case] = grid_model.best_params_\n",
    "            dict_best_params_l1[class_name] = dict_best_params_l2\n",
    "        dict_best_params[model_name] = dict_best_params_l1\n",
    "        \n",
    "    return dict_best_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08894764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_params = dict()\n",
    "\n",
    "# for pr_method, pr_data in dict_preproc_data.items():\n",
    "#     if pr_method not in dict_params.keys():\n",
    "#         split_dict = pr_data\n",
    "#         dict_params[pr_method] = grid_search(split_dict, classical_ml_models, classical_ml_params)\n",
    "#         with open('utils/dict_best_params', 'wb') as f:\n",
    "#             pickle.dump(dict_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e396f10",
   "metadata": {},
   "source": [
    "### modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9268309a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************  TF-IDF  *******************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1806c698ab55432d80731d2b20c1bbe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************  TF-IDF-SVD  *******************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0577cb341177479aabd3519b2bb0a3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************  word2vec  *******************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b8d187445844a687949ae9fec8ad9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************  BERT  *******************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1c5418717f4245bef4c5a09b927108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ros = RandomOverSampler(random_state=RS)\n",
    "\n",
    "for pr_method, pr_data in dict_preproc_data.items():\n",
    "    print(f'******************  {pr_method}  *******************')\n",
    "    pred_model_dict = dict()\n",
    "    \n",
    "    for model_name, model in tqdm(classical_ml_models.items()):\n",
    "\n",
    "        split_dict = pr_data\n",
    "        res_val_data = split_original_dict['val'].copy()\n",
    "        res_test_data = split_original_dict['test'].copy()\n",
    "\n",
    "        ros = RandomOverSampler(random_state=RS)\n",
    "\n",
    "        for class_name in [\"quarantine\", \"vaccines\",  \"masks\"]:\n",
    "            for case in ['stance', 'argument']:\n",
    "\n",
    "                x_ros, y_ros = ros.fit_resample(split_dict['train'], train_df[[f'{class_name}_{case}']])\n",
    "                logit = model\n",
    "                logit.fit(x_ros, y_ros)\n",
    "                res_val_data[f'{class_name}_{case}'] = logit.predict(split_dict['val'])\n",
    "                res_test_data[f'{class_name}_{case}'] = logit.predict(split_dict['test'])\n",
    "                \n",
    "        try:\n",
    "            pred_dict[pr_method][model_name] = {'val':res_val_data,\n",
    "                                           'test':res_test_data}  \n",
    "        except:\n",
    "\n",
    "            pred_dict[pr_method] = {model_name: {'val':res_val_data,\n",
    "                                           'test':res_test_data}}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13837759",
   "metadata": {},
   "source": [
    "## neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1277f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device='cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "973fcf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = TweetTokenizer()\n",
    "\n",
    "# def ling_preprocess(text):\n",
    "#     # TODO: add lemmatization\n",
    "#     # TODO: add bigrams\n",
    "#     return [e.lower() for e in tokenizer.tokenize(text)]\n",
    "\n",
    "# def create_vocab(dataset):  \n",
    "#     counter = Counter()\n",
    "#     for inst in dataset:\n",
    "#         counter.update(inst)\n",
    "\n",
    "#     return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af5ec8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.vocab import vocab as torch_vocab\n",
    "\n",
    "# all_tockens = []\n",
    "# for split in split_prepr_tockens_dict:\n",
    "#     all_tockens.extend(split_prepr_tockens_dict[split].tolist())\n",
    "\n",
    "# vocab = create_vocab(all_tockens)\n",
    "# sorted_by_freq_tuples = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "# ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "# v1 = torch_vocab(ordered_dict)\n",
    "# v1.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6d6dfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lens = [len(x) for x in split_prepr_text_dict['train']]\n",
    "\n",
    "# plt.hist(lens, bins=50)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfbfc616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomTextDataset(Dataset):\n",
    "#     def __init__(self, texts, labels, transform=None, target_transform=None, \n",
    "#                  use_padding=False, seq_len=100, pad_idx=-1):\n",
    "        \n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "#         self.transform = transform\n",
    "#         self.target_transform = target_transform\n",
    "        \n",
    "#         self.use_padding = use_padding\n",
    "#         self.seq_len = seq_len\n",
    "#         self.pad_idx = pad_idx\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.texts[idx]\n",
    "#         label = self.labels[idx]\n",
    "        \n",
    "#         if self.transform:\n",
    "#             text = self.transform(text)\n",
    "#         if self.use_padding:\n",
    "#             if len(text) < self.seq_len:\n",
    "#                 text.extend([self.pad_idx]*(self.seq_len - len(text)))\n",
    "#             else:\n",
    "#                 text = text[:self.seq_len]\n",
    "#         if self.target_transform:\n",
    "#             label = self.target_transform(label)\n",
    "            \n",
    "#         return torch.tensor(text), torch.tensor(label)\n",
    "    \n",
    "class EmbTextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, transform=None, target_transform=None, \n",
    "                  use_padding=False, seq_len=100, pad_idx=0):\n",
    "        \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        self.use_padding = use_padding\n",
    "        self.seq_len = seq_len\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.use_padding:\n",
    "            if len(text) < self.seq_len:\n",
    "                padding = np.zeros((self.seq_len - len(text), text.shape[1]))\n",
    "                text = np.vstack([padding, text])\n",
    "            else:\n",
    "                text = text[:self.seq_len]\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return torch.tensor(text, dtype=torch.float32), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "957dfe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1024)\n",
      "(1, 300)\n"
     ]
    }
   ],
   "source": [
    "model_bert.to(device) \n",
    "\n",
    "def embed_bert_cls_all(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, max_length=250, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    \n",
    "    embeddings = model_output.last_hidden_state\n",
    "    #print(embeddings.shape)\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()\n",
    "\n",
    "print(embed_bert_cls_all('привет', model_bert, tokenizer_bert).shape)\n",
    "\n",
    "\n",
    "def get_phrase_embedding(model, phrase_list):\n",
    "    \n",
    "    vector = np.zeros([len(phrase_list), model.vector_size], dtype='float32')\n",
    "    \n",
    "    for i, word in enumerate(phrase_list):\n",
    "        if word in model:\n",
    "            vector[i] = model.get_vector(word)\n",
    "            \n",
    "    return vector\n",
    "\n",
    "split_dict= {split:np.array([get_phrase_embedding(ru_fasttext_model, elem) for elem in data])\n",
    "            for split, data in split_text_dict.items()}\n",
    "vec = get_phrase_embedding(ru_fasttext_model, ['привет'])\n",
    "print(vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c83b2d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.2565301  -0.39395946  0.35733694 ... -0.4146366  -0.2918802\n",
      "   0.7296415 ]\n",
      " [-0.25757787 -0.4120124   0.54790694 ... -0.26167402 -0.50678337\n",
      "   0.04117091]\n",
      " [-0.47186714 -0.6157728   0.43466315 ... -0.46175832  0.0296714\n",
      "   0.36336938]\n",
      " [-0.6331686  -0.45733717  0.6162712  ... -0.58417386 -0.28518432\n",
      "   0.47437108]\n",
      " [-0.49425194 -0.29446265 -0.05818985 ... -0.45290542 -0.7587926\n",
      "   0.32994407]]\n"
     ]
    }
   ],
   "source": [
    "#text_pipeline = lambda x: v1(tokenizer.tokenize(x))\n",
    "text_pipeline = lambda x: embed_bert_cls_all(x, model_bert, tokenizer_bert)\n",
    "print(text_pipeline('Вакцина это плохо'))\n",
    "\n",
    "label_pipeline = lambda x: int(x[-1])\n",
    "label_pipeline_test = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b261525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07e6ad66c164af2a98c72888665b702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13896 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba31fad70f2349d7b36f15fc2ba11578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ae4516d9e14e2287666e4fe2c042e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edc278871004a46a12b17e992bd9bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13896 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66aeca0cf5224e1bbe0028a6eee18ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73512fa1c33e41fdbe5a68d3ce032d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f0f3b831b44e9687a6bedb875a0512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6ee00dbfde4abf84af5916563f036b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc38d5396494bd5bf66f939996e1fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4e5fa674bb49c199a579b8b75da378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06b224584cb476eaf57b6fb0a695fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd65baa17ba4011863cc4f2ce911413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cf294d94ea41f0b7b7be23c629ed84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10752 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8c019cb9c74bec907c2bdce42ed3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f777049d8447a4aea3df132a694166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4edb64a2b5fe437ab437dc3566b9beea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10752 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31f6ca93ac94224bef3120cfce5abd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a4db57a1444b69a2a74534728589b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ros = RandomOverSampler(random_state=RS)\n",
    "\n",
    "Resample = True     \n",
    "split_dict = split_prepr_text_dict\n",
    "BS = 64\n",
    "seq_len=100\n",
    "\n",
    "class_names = [\"quarantine\", \"vaccines\",  \"masks\"]\n",
    "cases = ['stance', 'argument']\n",
    "\n",
    "# class_names = [\"quarantine\"]\n",
    "# cases = ['stance']\n",
    "\n",
    "\n",
    "dict_dataloader = dict()\n",
    "dict_dataset = dict()\n",
    "\n",
    "for class_name in class_names:\n",
    "    dict_dataloader[class_name] = dict()\n",
    "    dict_dataset[class_name] = dict()\n",
    "    for case in cases:\n",
    "        dict_dataloader[class_name][case] = dict()\n",
    "        dict_dataset[class_name][case] = dict()\n",
    "        for split in split_dict:\n",
    "            \n",
    "            texts = split_prepr_text_dict[split]\n",
    "            labels = split_original_dict[split][f'{class_name}_{case}'].values\n",
    "            \n",
    "            if Resample and split=='train':\n",
    "                texts, labels = ros.fit_resample(texts.reshape(-1, 1), labels)\n",
    "                texts = [text_pipeline(text[0]) for text in tqdm(texts)]\n",
    "            else:\n",
    "                texts = [text_pipeline(text) for text in tqdm(texts)]\n",
    "            \n",
    "            target_transform = label_pipeline if split != 'test' else label_pipeline_test\n",
    "            dataset = EmbTextDataset(texts, labels, transform=text_pipeline, \n",
    "                                        target_transform=target_transform, use_padding=True, \n",
    "                                        seq_len=seq_len)\n",
    "            \n",
    "            shfl = True if split=='train' else False\n",
    "            \n",
    "            dict_dataset[class_name][case][split] = dataset\n",
    "            dict_dataloader[class_name][case][split] = DataLoader(dataset, batch_size=BS, \n",
    "                                                                  shuffle=shfl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a752a",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d574ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True )\n",
    "        \n",
    "        if bidirectional: \n",
    "            h_out = hidden_dim*2\n",
    "        else:\n",
    "            h_out = hidden_dim\n",
    "            \n",
    "        \n",
    "        self.fc1 = nn.Linear(h_out, hidden_dim//2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim//2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(text)\n",
    "        \n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.relu(self.fc1(hidden))\n",
    "        output = self.fc2(self.dropout(output))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "302215d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "LR = 0.001\n",
    "\n",
    "EMBEDDING_DIM = 1024\n",
    "HIDDEN_DIM = 1024\n",
    "OUTPUT_DIM = 4\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.2\n",
    "\n",
    "model = LSTM_net(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, \n",
    "                 N_LAYERS, BIDIRECTIONAL, DROPOUT).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00181f60",
   "metadata": {},
   "source": [
    "### train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "959082fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(loader):\n",
    "    global model\n",
    "    all_preds_number, correct_preds, losses, n_batches = 0, 0, 0, 0\n",
    "    model.train(True)\n",
    "    for x, labels in loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        y_out = model(x)\n",
    "        loss = criterion(y_out, labels)\n",
    "        losses += loss\n",
    "        y_out_labels = torch.log_softmax(y_out, dim = 1)\n",
    "        y_out_labels = torch.argmax(y_out_labels, dim = 1)\n",
    "\n",
    "        all_preds_number += labels.size(0)\n",
    "        correct_preds += (y_out_labels == labels).sum().item()\n",
    "\n",
    "        n_batches += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_loss = losses / n_batches\n",
    "    mean_acc = correct_preds / all_preds_number\n",
    "\n",
    "    if 'scheduler' in globals():\n",
    "        scheduler.step()\n",
    "    \n",
    "    return mean_loss, mean_acc \n",
    "\n",
    "def valid_step(loader):\n",
    "    global model\n",
    "    all_preds_number, correct_preds, losses, n_batches = 0, 0, 0, 0\n",
    "    \n",
    "    model.eval()    \n",
    "    for x, labels in loader:\n",
    "        with torch.no_grad():\n",
    "            x = x.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            y_out = model(x)\n",
    "            loss = criterion(y_out, labels)\n",
    "            losses += loss\n",
    "            y_out_labels = torch.log_softmax(y_out, dim = 1)\n",
    "            y_out_labels = torch.argmax(y_out_labels, dim = 1)\n",
    "\n",
    "            all_preds_number += labels.size(0)\n",
    "            correct_preds += (y_out_labels == labels).sum().item()\n",
    "\n",
    "            n_batches += 1\n",
    "\n",
    "    mean_loss = losses / n_batches\n",
    "    mean_acc = correct_preds / all_preds_number\n",
    "\n",
    "    return mean_loss, mean_acc \n",
    "\n",
    "def test_and_valid_step(test_dataset, valid_dataset):\n",
    "    global model\n",
    "    model.eval()    \n",
    "    with torch.no_grad():\n",
    "        test_dataset = torch.tensor(test_dataset).to(device)\n",
    "        valid_dataset = torch.tensor(valid_dataset).to(device)\n",
    "\n",
    "        y_test = model(test_dataset)\n",
    "        y_test_labels = torch.log_softmax(y_test, dim = 1)\n",
    "        y_test_labels = torch.argmax(y_test_labels, dim = 1)\n",
    "        \n",
    "        y_valid = model(valid_dataset)\n",
    "        y_valid_labels = torch.log_softmax(y_valid, dim = 1)\n",
    "        y_valid_labels = torch.argmax(y_valid_labels, dim = 1)\n",
    "\n",
    "    return y_test_labels, y_valid_labels \n",
    "\n",
    "\n",
    "def test_and_valid_step_emdeddings(test_dataloader, valid_dataloader):\n",
    "    global model\n",
    "    \n",
    "    valid_preds = []\n",
    "    test_preds = []\n",
    "    \n",
    "    model.eval()    \n",
    "    for x, labels in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            x = x.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            y_out = model(x)\n",
    "            y_out_labels = torch.log_softmax(y_out, dim = 1)\n",
    "            y_out_labels = torch.argmax(y_out_labels, dim = 1)\n",
    "            \n",
    "            test_preds.extend(y_out_labels.cpu().numpy().tolist())\n",
    "        \n",
    "    for x, labels in valid_dataloader:\n",
    "        with torch.no_grad():\n",
    "            x = x.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            y_out = model(x)\n",
    "            y_out_labels = torch.log_softmax(y_out, dim = 1)\n",
    "            y_out_labels = torch.argmax(y_out_labels, dim = 1)\n",
    "            \n",
    "            valid_preds.extend(y_out_labels.cpu().numpy().tolist())\n",
    "    \n",
    "    return test_preds, valid_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "320abb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************* stance quarantine **********************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8155489d74e6476281b0da35b06c4c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train loss: 0.648; train acc: 0.72; test loss: 0.440; test acc: 0.84\n",
      "[Epoch 2] train loss: 0.218; train acc: 0.92; test loss: 0.523; test acc: 0.85\n",
      "[Epoch 3] train loss: 0.114; train acc: 0.96; test loss: 0.679; test acc: 0.85\n",
      "[Epoch 4] train loss: 0.069; train acc: 0.98; test loss: 0.762; test acc: 0.86\n",
      "[Epoch 5] train loss: 0.013; train acc: 1.00; test loss: 0.819; test acc: 0.87\n",
      "[Epoch 6] train loss: 0.005; train acc: 1.00; test loss: 0.884; test acc: 0.87\n",
      "[Epoch 7] train loss: 0.003; train acc: 1.00; test loss: 0.944; test acc: 0.87\n",
      "[Epoch 8] train loss: 0.002; train acc: 1.00; test loss: 0.968; test acc: 0.87\n",
      "[Epoch 9] train loss: 0.002; train acc: 1.00; test loss: 0.982; test acc: 0.87\n",
      "[Epoch 10] train loss: 0.001; train acc: 1.00; test loss: 0.990; test acc: 0.87\n",
      "********************* argument quarantine **********************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b27300b36a4f6eb65b24a2a15517eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train loss: 0.514; train acc: 0.79; test loss: 0.375; test acc: 0.90\n",
      "[Epoch 2] train loss: 0.139; train acc: 0.95; test loss: 0.507; test acc: 0.87\n",
      "[Epoch 3] train loss: 0.321; train acc: 0.90; test loss: 0.408; test acc: 0.88\n",
      "[Epoch 4] train loss: 0.092; train acc: 0.97; test loss: 0.491; test acc: 0.90\n",
      "[Epoch 5] train loss: 0.032; train acc: 0.99; test loss: 0.521; test acc: 0.92\n",
      "[Epoch 6] train loss: 0.023; train acc: 0.99; test loss: 0.539; test acc: 0.92\n",
      "[Epoch 7] train loss: 0.019; train acc: 0.99; test loss: 0.566; test acc: 0.92\n",
      "[Epoch 8] train loss: 0.016; train acc: 1.00; test loss: 0.549; test acc: 0.92\n",
      "[Epoch 9] train loss: 0.013; train acc: 1.00; test loss: 0.564; test acc: 0.92\n",
      "[Epoch 10] train loss: 0.012; train acc: 1.00; test loss: 0.578; test acc: 0.92\n",
      "********************* stance vaccines **********************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8176beb78ff4102af75495dc6ae9584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train loss: 0.694; train acc: 0.69; test loss: 0.371; test acc: 0.87\n",
      "[Epoch 2] train loss: 0.275; train acc: 0.91; test loss: 0.645; test acc: 0.81\n",
      "[Epoch 3] train loss: 0.090; train acc: 0.97; test loss: 0.825; test acc: 0.86\n",
      "[Epoch 4] train loss: 0.053; train acc: 0.98; test loss: 0.699; test acc: 0.87\n",
      "[Epoch 5] train loss: 0.009; train acc: 1.00; test loss: 0.840; test acc: 0.88\n",
      "[Epoch 6] train loss: 0.003; train acc: 1.00; test loss: 0.875; test acc: 0.89\n",
      "[Epoch 7] train loss: 0.002; train acc: 1.00; test loss: 0.899; test acc: 0.88\n",
      "[Epoch 8] train loss: 0.002; train acc: 1.00; test loss: 0.945; test acc: 0.88\n",
      "[Epoch 9] train loss: 0.001; train acc: 1.00; test loss: 0.939; test acc: 0.89\n",
      "[Epoch 10] train loss: 0.001; train acc: 1.00; test loss: 0.946; test acc: 0.89\n",
      "********************* argument vaccines **********************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d945a65ebc4baf9deb3ba48fdc11e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train loss: 0.630; train acc: 0.73; test loss: 0.336; test acc: 0.91\n",
      "[Epoch 2] train loss: 0.161; train acc: 0.95; test loss: 0.403; test acc: 0.91\n",
      "[Epoch 3] train loss: 0.065; train acc: 0.98; test loss: 0.482; test acc: 0.91\n",
      "[Epoch 4] train loss: 0.252; train acc: 0.89; test loss: 1.238; test acc: 0.76\n",
      "[Epoch 5] train loss: 1.377; train acc: 0.30; test loss: 1.328; test acc: 0.65\n",
      "[Epoch 6] train loss: 1.268; train acc: 0.41; test loss: 0.842; test acc: 0.71\n",
      "[Epoch 7] train loss: 1.053; train acc: 0.50; test loss: 0.710; test acc: 0.74\n",
      "[Epoch 8] train loss: 0.970; train acc: 0.54; test loss: 0.522; test acc: 0.80\n",
      "[Epoch 9] train loss: 0.919; train acc: 0.58; test loss: 0.552; test acc: 0.78\n",
      "[Epoch 10] train loss: 0.908; train acc: 0.58; test loss: 0.534; test acc: 0.79\n",
      "********************* stance masks **********************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b87d48279774dd081504cda7544589d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train loss: 0.841; train acc: 0.61; test loss: 0.496; test acc: 0.79\n",
      "[Epoch 2] train loss: 0.531; train acc: 0.79; test loss: 0.553; test acc: 0.79\n",
      "[Epoch 3] train loss: 0.421; train acc: 0.86; test loss: 0.861; test acc: 0.75\n",
      "[Epoch 4] train loss: 0.178; train acc: 0.94; test loss: 0.729; test acc: 0.81\n",
      "[Epoch 5] train loss: 0.060; train acc: 0.98; test loss: 0.956; test acc: 0.81\n",
      "[Epoch 6] train loss: 0.031; train acc: 0.99; test loss: 1.088; test acc: 0.80\n",
      "[Epoch 7] train loss: 0.020; train acc: 0.99; test loss: 1.210; test acc: 0.81\n",
      "[Epoch 8] train loss: 0.013; train acc: 1.00; test loss: 1.349; test acc: 0.81\n",
      "[Epoch 9] train loss: 0.008; train acc: 1.00; test loss: 1.351; test acc: 0.81\n",
      "[Epoch 10] train loss: 0.007; train acc: 1.00; test loss: 1.386; test acc: 0.81\n",
      "********************* argument masks **********************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4a1cf7f06b48f984e2fd6c1cb888e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train loss: 0.698; train acc: 0.69; test loss: 0.498; test acc: 0.81\n",
      "[Epoch 2] train loss: 0.266; train acc: 0.91; test loss: 0.583; test acc: 0.83\n",
      "[Epoch 3] train loss: 0.127; train acc: 0.96; test loss: 0.717; test acc: 0.85\n",
      "[Epoch 4] train loss: 0.095; train acc: 0.97; test loss: 0.637; test acc: 0.87\n",
      "[Epoch 5] train loss: 0.021; train acc: 0.99; test loss: 0.748; test acc: 0.89\n",
      "[Epoch 6] train loss: 0.010; train acc: 1.00; test loss: 0.829; test acc: 0.89\n",
      "[Epoch 7] train loss: 0.006; train acc: 1.00; test loss: 0.941; test acc: 0.89\n",
      "[Epoch 8] train loss: 0.005; train acc: 1.00; test loss: 0.944; test acc: 0.88\n",
      "[Epoch 9] train loss: 0.004; train acc: 1.00; test loss: 0.923; test acc: 0.89\n",
      "[Epoch 10] train loss: 0.003; train acc: 1.00; test loss: 0.939; test acc: 0.89\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # [\"quarantine\", \"vaccines\",  \"masks\"]\n",
    "# # ['stance', 'argument']\n",
    "# class_name = 'quarantine'\n",
    "# case = 'stance'\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'LSTM'\n",
    "emb_type = 'BERT'\n",
    "\n",
    "pred_model_dict = dict()\n",
    "\n",
    "res_val_data = split_original_dict['val'].copy()\n",
    "res_test_data = split_original_dict['test'].copy()\n",
    "\n",
    "pred_model_dict = dict()\n",
    "\n",
    "for class_name in [\"quarantine\", \"vaccines\",  \"masks\"]:\n",
    "    for case in ['stance', 'argument']:\n",
    "        print(f'********************* {case} {class_name} **********************')\n",
    "        \n",
    "        model = LSTM_net(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, \n",
    "                 N_LAYERS, BIDIRECTIONAL, DROPOUT).to(device)\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=0.0001)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.1)\n",
    "        \n",
    "        train_loader = dict_dataloader[class_name][case]['train']\n",
    "        valid_loader = dict_dataloader[class_name][case]['val']\n",
    "        test_loader = dict_dataloader[class_name][case]['test']\n",
    " \n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "                train_loss, train_acc = train_step(train_loader)\n",
    "                test_loss, test_acc = valid_step(valid_loader) \n",
    "\n",
    "\n",
    "                print(f'[Epoch {epoch + 1}] train loss: {train_loss:.3f}; train acc: {train_acc:.2f}; ' + \n",
    "                      f'test loss: {test_loss:.3f}; test acc: {test_acc:.2f}')\n",
    "\n",
    "                test_preds, val_preds = test_and_valid_step_emdeddings(test_loader, valid_loader)\n",
    "\n",
    "                res_test_data[f'{class_name}_{case}'] = np.array(test_preds) - 1\n",
    "                res_val_data[f'{class_name}_{case}'] = np.array(val_preds) - 1\n",
    "        try:\n",
    "            pred_dict[emb_type][model_name] = {'val':res_val_data,\n",
    "                                           'test':res_test_data}  \n",
    "        except:\n",
    "\n",
    "            pred_dict[emb_type] = {model_name: {'val':res_val_data,\n",
    "                                           'test':res_test_data}}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55feaf17",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1360bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels, target_transform=None):\n",
    "        \n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embeddings = self.embeddings[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return torch.FloatTensor(embeddings), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e784f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=RS)\n",
    "\n",
    "Resample = True     \n",
    "split_dict = split_prepr_text_dict\n",
    "BS = 64\n",
    "\n",
    "emb_type= 'TF-IDF'\n",
    "\n",
    "dict_dataloader = dict()\n",
    "for class_name in [\"quarantine\", \"vaccines\",  \"masks\"]:\n",
    "    dict_dataloader[class_name] = dict()\n",
    "    for case in ['stance', 'argument']:\n",
    "        dict_dataloader[class_name][case] = dict()\n",
    "        for split in split_dict:\n",
    "            \n",
    "            texts = dict_preproc_data[emb_type][split]\n",
    "            if emb_type == 'TF-IDF':\n",
    "                texts = texts.toarray()\n",
    "            labels = split_original_dict[split][f'{class_name}_{case}'].values\n",
    "            \n",
    "            if Resample and split=='train':\n",
    "                texts, labels = ros.fit_resample(texts, labels)\n",
    "            \n",
    "            dataset = TextDataset(texts, labels, target_transform=label_pipeline)\n",
    "            \n",
    "            shfl = True if split=='train' else False\n",
    "            dict_dataloader[class_name][case][split] = DataLoader(dataset, batch_size=BS, \n",
    "                                                                  shuffle=shfl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e961559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_net(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        output = self.fc1(text)\n",
    "        output = self.fc2(self.dropout(self.relu(output)))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "565b686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 6 if emb_type == 'TF-IDF' else 15\n",
    "LR = 0.001\n",
    "\n",
    "EMBEDDING_DIM = texts.shape[1]\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 4\n",
    "DROPOUT = 0.2\n",
    "\n",
    "model = MLP_net(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2fc6ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_valid_step(test_dataset, valid_dataset):\n",
    "    \n",
    "    model.eval()    \n",
    "    with torch.no_grad():\n",
    "        test_dataset = torch.tensor(test_dataset, dtype=torch.float32).to(device)\n",
    "        valid_dataset = torch.tensor(valid_dataset, dtype=torch.float32).to(device)\n",
    "\n",
    "        y_test = model(test_dataset)\n",
    "        y_test_labels = torch.log_softmax(y_test, dim = 1)\n",
    "        y_test_labels = torch.argmax(y_test_labels, dim = 1)\n",
    "        \n",
    "        y_valid = model(valid_dataset)\n",
    "        y_valid_labels = torch.log_softmax(y_valid, dim = 1)\n",
    "        y_valid_labels = torch.argmax(y_valid_labels, dim = 1)\n",
    "\n",
    "    return y_test_labels, y_valid_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "321ef739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************* stance quarantine **********************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b50a71b09b3471180fd4da3853187a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train loss: 0.588; train acc: 0.91; test loss: 0.379; test acc: 0.88\n",
      "[Epoch 2] train loss: 0.020; train acc: 1.00; test loss: 0.402; test acc: 0.87\n",
      "[Epoch 3] train loss: 0.005; train acc: 1.00; test loss: 0.415; test acc: 0.88\n",
      "[Epoch 4] train loss: 0.002; train acc: 1.00; test loss: 0.439; test acc: 0.88\n",
      "[Epoch 5] train loss: 0.001; train acc: 1.00; test loss: 0.468; test acc: 0.88\n",
      "[Epoch 6] train loss: 0.001; train acc: 1.00; test loss: 0.482; test acc: 0.88\n",
      "********************* argument quarantine **********************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6a4dff0eb3438bbed818c697b39d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train loss: 0.529; train acc: 0.92; test loss: 0.312; test acc: 0.92\n",
      "[Epoch 2] train loss: 0.020; train acc: 1.00; test loss: 0.314; test acc: 0.92\n",
      "[Epoch 3] train loss: 0.005; train acc: 1.00; test loss: 0.329; test acc: 0.92\n",
      "[Epoch 4] train loss: 0.003; train acc: 1.00; test loss: 0.338; test acc: 0.92\n",
      "[Epoch 5] train loss: 0.002; train acc: 1.00; test loss: 0.345; test acc: 0.92\n",
      "[Epoch 6] train loss: 0.002; train acc: 1.00; test loss: 0.357; test acc: 0.92\n",
      "********************* stance vaccines **********************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f4b9532ea346abb6017d9f5e0457c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train loss: 0.542; train acc: 0.92; test loss: 0.287; test acc: 0.89\n",
      "[Epoch 2] train loss: 0.013; train acc: 1.00; test loss: 0.289; test acc: 0.89\n",
      "[Epoch 3] train loss: 0.004; train acc: 1.00; test loss: 0.295; test acc: 0.89\n",
      "[Epoch 4] train loss: 0.002; train acc: 1.00; test loss: 0.304; test acc: 0.89\n",
      "[Epoch 5] train loss: 0.001; train acc: 1.00; test loss: 0.316; test acc: 0.89\n",
      "[Epoch 6] train loss: 0.001; train acc: 1.00; test loss: 0.324; test acc: 0.89\n",
      "********************* argument vaccines **********************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f795a971cd894740b277de51a7e3f72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train loss: 0.498; train acc: 0.91; test loss: 0.249; test acc: 0.92\n",
      "[Epoch 2] train loss: 0.012; train acc: 1.00; test loss: 0.256; test acc: 0.92\n",
      "[Epoch 3] train loss: 0.003; train acc: 1.00; test loss: 0.265; test acc: 0.92\n",
      "[Epoch 4] train loss: 0.001; train acc: 1.00; test loss: 0.285; test acc: 0.92\n",
      "[Epoch 5] train loss: 0.001; train acc: 1.00; test loss: 0.299; test acc: 0.92\n",
      "[Epoch 6] train loss: 0.000; train acc: 1.00; test loss: 0.312; test acc: 0.92\n",
      "********************* stance masks **********************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baac50654c724c6ba1bee2380c9e1b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train loss: 0.849; train acc: 0.80; test loss: 0.529; test acc: 0.80\n",
      "[Epoch 2] train loss: 0.070; train acc: 1.00; test loss: 0.517; test acc: 0.81\n",
      "[Epoch 3] train loss: 0.013; train acc: 1.00; test loss: 0.540; test acc: 0.81\n",
      "[Epoch 4] train loss: 0.005; train acc: 1.00; test loss: 0.568; test acc: 0.81\n",
      "[Epoch 5] train loss: 0.003; train acc: 1.00; test loss: 0.591; test acc: 0.81\n",
      "[Epoch 6] train loss: 0.002; train acc: 1.00; test loss: 0.610; test acc: 0.80\n",
      "********************* argument masks **********************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cea321d0d3143f29760e671f1b0a8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train loss: 0.744; train acc: 0.88; test loss: 0.423; test acc: 0.87\n",
      "[Epoch 2] train loss: 0.051; train acc: 1.00; test loss: 0.387; test acc: 0.88\n",
      "[Epoch 3] train loss: 0.009; train acc: 1.00; test loss: 0.395; test acc: 0.88\n",
      "[Epoch 4] train loss: 0.004; train acc: 1.00; test loss: 0.409; test acc: 0.88\n",
      "[Epoch 5] train loss: 0.002; train acc: 1.00; test loss: 0.430; test acc: 0.88\n",
      "[Epoch 6] train loss: 0.001; train acc: 1.00; test loss: 0.449; test acc: 0.88\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res_val_data = split_original_dict['val'].copy()\n",
    "res_test_data = split_original_dict['test'].copy()\n",
    "\n",
    "model_name = 'MLP'\n",
    "pred_model_dict = dict()\n",
    "\n",
    "for class_name in [\"quarantine\", \"vaccines\",  \"masks\"]:\n",
    "    for case in ['stance', 'argument']:\n",
    "        print(f'********************* {case} {class_name} **********************')\n",
    "        \n",
    "        model = MLP_net(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT).to(device)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 20, gamma=0.1)\n",
    "        train_loader = dict_dataloader[class_name][case]['train']\n",
    "        valid_loader = dict_dataloader[class_name][case]['val']\n",
    "\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "                train_loss, train_acc = train_step(train_loader)\n",
    "                test_loss, test_acc = valid_step(valid_loader) \n",
    "\n",
    "                print(f'[Epoch {epoch + 1}] train loss: {train_loss:.3f}; train acc: {train_acc:.2f}; ' + \n",
    "                      f'test loss: {test_loss:.3f}; test acc: {test_acc:.2f}')\n",
    "        \n",
    "        \n",
    "        test_dataset = dict_preproc_data[emb_type]['test']\n",
    "        valid_dataset = dict_preproc_data[emb_type]['val']\n",
    "        \n",
    "        if emb_type == 'TF-IDF':\n",
    "            test_dataset, valid_dataset = test_dataset.toarray(), valid_dataset.toarray()\n",
    "        test_preds, val_preds = test_and_valid_step(test_dataset, valid_dataset)\n",
    "\n",
    "        res_test_data[f'{class_name}_{case}'] = test_preds.cpu() - 1\n",
    "        res_val_data[f'{class_name}_{case}'] = val_preds.cpu() - 1\n",
    "        \n",
    "        try:\n",
    "            pred_dict[emb_type][model_name] = {'val':res_val_data,\n",
    "                                           'test':res_test_data}  \n",
    "        except:\n",
    "\n",
    "            pred_dict[emb_type] = {model_name: {'val':res_val_data,\n",
    "                                           'test':res_test_data}}    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed84f4",
   "metadata": {},
   "source": [
    "# 5. Evaluation and postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69ee13bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_metrics = pd.read_csv('results/metrics.csv')\n",
    "\n",
    "# with open('results/predictions.pickle', 'rb') as f:\n",
    "#     pred_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e8f37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocessing\n",
    "\n",
    "def postprocessing(res_val_data):\n",
    "    res_val_data = res_val_data.copy()\n",
    "    for col in res_val_data.columns:\n",
    "          if 'text' not in col:\n",
    "                res_val_data[col] = res_val_data[col].apply(lambda x : x[-1]).astype(int) - 1\n",
    "\n",
    "    for claim in [\"quarantine\", \"vaccines\",  \"masks\"]:\n",
    "        res_val_data.loc[(res_val_data[f'{claim}_stance'] == -1) & (res_val_data[f'{claim}_argument'] != -1), [f'{claim}_argument']] = -1\n",
    "        res_val_data.loc[(res_val_data[f'{claim}_stance'] > -1) & (res_val_data[f'{claim}_argument'] == -1), [f'{claim}_argument']] = 0\n",
    "\n",
    "\n",
    "    return res_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40e55031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masks_stance</th>\n",
       "      <th>masks_argument</th>\n",
       "      <th>quarantine_stance</th>\n",
       "      <th>quarantine_argument</th>\n",
       "      <th>vaccines_stance</th>\n",
       "      <th>vaccines_argument</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TF-IDF_xgb</th>\n",
       "      <td>0.502256</td>\n",
       "      <td>0.548416</td>\n",
       "      <td>0.447039</td>\n",
       "      <td>0.455734</td>\n",
       "      <td>0.466909</td>\n",
       "      <td>0.409654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF_rf</th>\n",
       "      <td>0.509735</td>\n",
       "      <td>0.411761</td>\n",
       "      <td>0.371292</td>\n",
       "      <td>0.301349</td>\n",
       "      <td>0.351586</td>\n",
       "      <td>0.329968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF_lr</th>\n",
       "      <td>0.558534</td>\n",
       "      <td>0.594784</td>\n",
       "      <td>0.505222</td>\n",
       "      <td>0.422629</td>\n",
       "      <td>0.510718</td>\n",
       "      <td>0.443939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF_svm</th>\n",
       "      <td>0.493312</td>\n",
       "      <td>0.411916</td>\n",
       "      <td>0.37219</td>\n",
       "      <td>0.31688</td>\n",
       "      <td>0.374336</td>\n",
       "      <td>0.326963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF_knn</th>\n",
       "      <td>0.384852</td>\n",
       "      <td>0.388647</td>\n",
       "      <td>0.325224</td>\n",
       "      <td>0.291057</td>\n",
       "      <td>0.334587</td>\n",
       "      <td>0.330484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF_MLP</th>\n",
       "      <td>0.520583</td>\n",
       "      <td>0.502768</td>\n",
       "      <td>0.437212</td>\n",
       "      <td>0.395003</td>\n",
       "      <td>0.507747</td>\n",
       "      <td>0.41713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF-SVD_xgb</th>\n",
       "      <td>0.287866</td>\n",
       "      <td>0.274718</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.062523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF-SVD_rf</th>\n",
       "      <td>0.199476</td>\n",
       "      <td>0.252157</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF-SVD_lr</th>\n",
       "      <td>0.23371</td>\n",
       "      <td>0.211141</td>\n",
       "      <td>0.18327</td>\n",
       "      <td>0.159885</td>\n",
       "      <td>0.020581</td>\n",
       "      <td>0.016487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF-SVD_svm</th>\n",
       "      <td>0.076577</td>\n",
       "      <td>0.085517</td>\n",
       "      <td>0.007641</td>\n",
       "      <td>0.00431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF-SVD_knn</th>\n",
       "      <td>0.171164</td>\n",
       "      <td>0.192956</td>\n",
       "      <td>0.026122</td>\n",
       "      <td>0.023347</td>\n",
       "      <td>0.102165</td>\n",
       "      <td>0.107634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec_xgb</th>\n",
       "      <td>0.262021</td>\n",
       "      <td>0.257645</td>\n",
       "      <td>0.216072</td>\n",
       "      <td>0.179253</td>\n",
       "      <td>0.188664</td>\n",
       "      <td>0.170124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec_rf</th>\n",
       "      <td>0.171681</td>\n",
       "      <td>0.198458</td>\n",
       "      <td>0.095085</td>\n",
       "      <td>0.073371</td>\n",
       "      <td>0.031884</td>\n",
       "      <td>0.030159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec_lr</th>\n",
       "      <td>0.313795</td>\n",
       "      <td>0.31449</td>\n",
       "      <td>0.257632</td>\n",
       "      <td>0.201836</td>\n",
       "      <td>0.21376</td>\n",
       "      <td>0.21304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec_svm</th>\n",
       "      <td>0.266496</td>\n",
       "      <td>0.281394</td>\n",
       "      <td>0.188768</td>\n",
       "      <td>0.157269</td>\n",
       "      <td>0.199066</td>\n",
       "      <td>0.184052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec_knn</th>\n",
       "      <td>0.19825</td>\n",
       "      <td>0.213481</td>\n",
       "      <td>0.196849</td>\n",
       "      <td>0.180171</td>\n",
       "      <td>0.143806</td>\n",
       "      <td>0.151826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT_xgb</th>\n",
       "      <td>0.452702</td>\n",
       "      <td>0.47978</td>\n",
       "      <td>0.408762</td>\n",
       "      <td>0.304981</td>\n",
       "      <td>0.425091</td>\n",
       "      <td>0.310831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT_rf</th>\n",
       "      <td>0.376048</td>\n",
       "      <td>0.360333</td>\n",
       "      <td>0.285348</td>\n",
       "      <td>0.268533</td>\n",
       "      <td>0.308258</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT_lr</th>\n",
       "      <td>0.484418</td>\n",
       "      <td>0.529374</td>\n",
       "      <td>0.502515</td>\n",
       "      <td>0.451177</td>\n",
       "      <td>0.494172</td>\n",
       "      <td>0.434966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT_svm</th>\n",
       "      <td>0.502895</td>\n",
       "      <td>0.550555</td>\n",
       "      <td>0.482443</td>\n",
       "      <td>0.478809</td>\n",
       "      <td>0.493748</td>\n",
       "      <td>0.456643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT_knn</th>\n",
       "      <td>0.375508</td>\n",
       "      <td>0.392414</td>\n",
       "      <td>0.326773</td>\n",
       "      <td>0.339735</td>\n",
       "      <td>0.325134</td>\n",
       "      <td>0.323134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT_LSTM</th>\n",
       "      <td>0.499304</td>\n",
       "      <td>0.537058</td>\n",
       "      <td>0.453505</td>\n",
       "      <td>0.430544</td>\n",
       "      <td>0.49339</td>\n",
       "      <td>0.287423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               masks_stance masks_argument quarantine_stance  \\\n",
       "TF-IDF_xgb         0.502256       0.548416          0.447039   \n",
       "TF-IDF_rf          0.509735       0.411761          0.371292   \n",
       "TF-IDF_lr          0.558534       0.594784          0.505222   \n",
       "TF-IDF_svm         0.493312       0.411916           0.37219   \n",
       "TF-IDF_knn         0.384852       0.388647          0.325224   \n",
       "TF-IDF_MLP         0.520583       0.502768          0.437212   \n",
       "TF-IDF-SVD_xgb     0.287866       0.274718          0.047059   \n",
       "TF-IDF-SVD_rf      0.199476       0.252157          0.005495   \n",
       "TF-IDF-SVD_lr       0.23371       0.211141           0.18327   \n",
       "TF-IDF-SVD_svm     0.076577       0.085517          0.007641   \n",
       "TF-IDF-SVD_knn     0.171164       0.192956          0.026122   \n",
       "word2vec_xgb       0.262021       0.257645          0.216072   \n",
       "word2vec_rf        0.171681       0.198458          0.095085   \n",
       "word2vec_lr        0.313795        0.31449          0.257632   \n",
       "word2vec_svm       0.266496       0.281394          0.188768   \n",
       "word2vec_knn        0.19825       0.213481          0.196849   \n",
       "BERT_xgb           0.452702        0.47978          0.408762   \n",
       "BERT_rf            0.376048       0.360333          0.285348   \n",
       "BERT_lr            0.484418       0.529374          0.502515   \n",
       "BERT_svm           0.502895       0.550555          0.482443   \n",
       "BERT_knn           0.375508       0.392414          0.326773   \n",
       "BERT_LSTM          0.499304       0.537058          0.453505   \n",
       "\n",
       "               quarantine_argument vaccines_stance vaccines_argument  \n",
       "TF-IDF_xgb                0.455734        0.466909          0.409654  \n",
       "TF-IDF_rf                 0.301349        0.351586          0.329968  \n",
       "TF-IDF_lr                 0.422629        0.510718          0.443939  \n",
       "TF-IDF_svm                 0.31688        0.374336          0.326963  \n",
       "TF-IDF_knn                0.291057        0.334587          0.330484  \n",
       "TF-IDF_MLP                0.395003        0.507747           0.41713  \n",
       "TF-IDF-SVD_xgb            0.062523             0.0               0.0  \n",
       "TF-IDF-SVD_rf             0.001468             0.0               0.0  \n",
       "TF-IDF-SVD_lr             0.159885        0.020581          0.016487  \n",
       "TF-IDF-SVD_svm             0.00431             0.0               0.0  \n",
       "TF-IDF-SVD_knn            0.023347        0.102165          0.107634  \n",
       "word2vec_xgb              0.179253        0.188664          0.170124  \n",
       "word2vec_rf               0.073371        0.031884          0.030159  \n",
       "word2vec_lr               0.201836         0.21376           0.21304  \n",
       "word2vec_svm              0.157269        0.199066          0.184052  \n",
       "word2vec_knn              0.180171        0.143806          0.151826  \n",
       "BERT_xgb                  0.304981        0.425091          0.310831  \n",
       "BERT_rf                   0.268533        0.308258          0.222222  \n",
       "BERT_lr                   0.451177        0.494172          0.434966  \n",
       "BERT_svm                  0.478809        0.493748          0.456643  \n",
       "BERT_knn                  0.339735        0.325134          0.323134  \n",
       "BERT_LSTM                 0.430544         0.49339          0.287423  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "val_post = postprocessing(val_df)\n",
    "df_metrics = pd.DataFrame([], columns = [col for col in train_df.columns if 'text' not in col])\n",
    "\n",
    "for pr_method, pr_data in pred_dict.items():\n",
    "    for model_name, model_pred in pr_data.items():\n",
    "        \n",
    "        y_pred_table = pred_dict[pr_method][model_name]['val'].copy()\n",
    "        if model_name not in ['MLP', 'LSTM']:\n",
    "            y_pred_table = postprocessing(y_pred_table)\n",
    "        \n",
    "        for case in ['stance', 'argument']:\n",
    "            \n",
    "            for class_name in [\"quarantine\", \"vaccines\",  \"masks\"]:\n",
    "                y_true = val_post[f'{class_name}_{case}']\n",
    "                y_pred = y_pred_table[f'{class_name}_{case}']\n",
    "                #f1 = accuracy_score(y_true, y_pred) #, labels=[2, 1, 0], average=\"macro\")\n",
    "                #f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "                f1 = f1_score(y_true, y_pred, labels=[2, 1, 0], average=\"macro\")\n",
    "                \n",
    "                df_metrics.loc[f'{pr_method}_{model_name}', f'{class_name}_{case}'] = f1\n",
    "df_metrics            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d4760ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_metrics.to_csv('results/metrics.csv')\n",
    "\n",
    "# with open('results/predictions.pickle', 'wb') as f:\n",
    "#     pickle.dump(pred_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64105e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'masks_stance': {'method': 'TF-IDF', 'model': 'lr'},\n",
       " 'masks_argument': {'method': 'TF-IDF', 'model': 'lr'},\n",
       " 'quarantine_stance': {'method': 'TF-IDF', 'model': 'lr'},\n",
       " 'quarantine_argument': {'method': 'BERT', 'model': 'svm'},\n",
       " 'vaccines_stance': {'method': 'TF-IDF', 'model': 'lr'},\n",
       " 'vaccines_argument': {'method': 'BERT', 'model': 'svm'}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_best_methods = {col:df_metrics.index[np.argmax(df_metrics[col])] for col in df_metrics.columns}\n",
    "dict_best_methods = {col:{'method':best_meth.split('_')[0], 'model':best_meth.split('_')[1]} \n",
    "                     for col, best_meth in dict_best_methods.items()}\n",
    "dict_best_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f78bcdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masks_stance</th>\n",
       "      <th>masks_argument</th>\n",
       "      <th>quarantine_stance</th>\n",
       "      <th>quarantine_argument</th>\n",
       "      <th>vaccines_stance</th>\n",
       "      <th>vaccines_argument</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   masks_stance  masks_argument  quarantine_stance  quarantine_argument  \\\n",
       "0            -1              -1                  1                    1   \n",
       "1            -1              -1                  2                    1   \n",
       "2            -1              -1                  2                    1   \n",
       "3            -1              -1                  2                    2   \n",
       "4            -1              -1                  2                    1   \n",
       "\n",
       "   vaccines_stance  vaccines_argument  \n",
       "0               -1                 -1  \n",
       "1               -1                 -1  \n",
       "2               -1                 -1  \n",
       "3               -1                 -1  \n",
       "4               -1                 -1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_to_submit = pd.DataFrame([])\n",
    "\n",
    "for col, best_dict in dict_best_methods.items():\n",
    "    \n",
    "    y_pred_table = pred_dict[best_dict['method']]\n",
    "    if best_dict['model'] not in ['MLP', 'LSTM']:\n",
    "        y_pred_table = postprocessing(y_pred_table[best_dict['model']]['test'].copy())\n",
    "    else:\n",
    "        y_pred_table = y_pred_table[best_dict['model']]['test'].copy()\n",
    "    result_to_submit[col] = y_pred_table[col]\n",
    "    \n",
    "result_to_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d2af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_to_submit.to_csv(f'results/final_submit.tsv', index = False, sep='\\t')\n",
    "import os\n",
    "os.chdir('results')\n",
    "!zip \"final_submit.zip\" \"final_submit.tsv\"\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec923d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('results/first_submit.tsv', sep='\\t')\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # postprocessing\n",
    "# for col in res_test_data.columns:\n",
    "#       if 'text' not in col:\n",
    "#             #res_data[col] = res_data[col].astype(int) - 1\n",
    "#             res_val_data[col] = res_val_data[col].apply(lambda x : x[-1]).astype(int) - 1\n",
    "#             res_test_data[col] = res_test_data[col].apply(lambda x : x[-1]).astype(int) - 1\n",
    "\n",
    "# for claim in [\"quarantine\", \"vaccines\",  \"masks\"]:\n",
    "#     res_val_data.loc[(res_val_data[f'{claim}_stance'] == -1) & (res_val_data[f'{claim}_argument'] != -1), [f'{claim}_argument']] = -1\n",
    "#     res_val_data.loc[(res_val_data[f'{claim}_stance'] > -1) & (res_val_data[f'{claim}_argument'] == -1), [f'{claim}_argument']] = 0\n",
    "#     res_test_data.loc[(res_test_data[f'{claim}_stance'] == -1) & (res_test_data[f'{claim}_argument'] != -1), [f'{claim}_argument']] = -1\n",
    "#     res_test_data.loc[(res_test_data[f'{claim}_stance'] > -1) & (res_test_data[f'{claim}_argument'] == -1), [f'{claim}_argument']] = 0\n",
    "\n",
    "    \n",
    "# res_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad10c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "assignment-template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
